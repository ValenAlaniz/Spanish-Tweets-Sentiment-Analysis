{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-4azOYi8KSoC"
   },
   "source": [
    "# Laboratorio de Introducción al Procesamiento de Lenguaje Natural\n",
    "\n",
    "El objetivo de este laboratorio es realizar diferentes experimentos para representar y clasificar textos. Para esto se trabajará con un corpus para análisis de sentimiento, creado para la competencia [TASS 2020](http://www.sepln.org/workshops/tass/) (IberLEF - SEPLN).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAumcYFLP0f8"
   },
   "source": [
    "## Parte 1 - Carga y preprocesamiento del corpus\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ix076JBqADzb"
   },
   "source": [
    "### Carga"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "nnBJGtH5QLcA"
   },
   "outputs": [],
   "source": [
    "# Carga de los datasets\n",
    "import csv\n",
    "\n",
    "# Carga del archivo train.csv\n",
    "with open('datos/train.csv', newline='', encoding=\"utf-8\") as corpus_csv:\n",
    "    reader = csv.reader(corpus_csv)\n",
    "    next(reader) # Saltea el cabezal del archivo\n",
    "    train_set = [x for x in reader]\n",
    "\n",
    "# Carga del archivo devel.csv\n",
    "with open('datos/devel.csv', newline='', encoding=\"utf-8\") as corpus_csv:\n",
    "    reader = csv.reader(corpus_csv)\n",
    "    next(reader) # Saltea el cabezal del archivo\n",
    "    devel_set = [x for x in reader]\n",
    "\n",
    "# Carga del archivo test.csv\n",
    "with open('datos/test.csv', newline='', encoding=\"utf-8\") as corpus_csv:\n",
    "    reader = csv.reader(corpus_csv)\n",
    "    next(reader) # Saltea el cabezal del archivo\n",
    "    test_set = [x for x in reader]\n",
    "\n",
    "# Carga del archivo stop_words_esp_anasent.csv\n",
    "with open('datos/stop_words_esp_anasent.csv', newline='', encoding=\"utf-8\") as corpus_csv:\n",
    "    reader = csv.reader(corpus_csv)\n",
    "    next(reader) # Saltea el cabezal del archivo\n",
    "    stop_words_esp_anasent = [x[0] for x in reader]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wt3Ow8D8AIyw"
   },
   "source": [
    "### Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "tjjY9pRHZ7JH"
   },
   "outputs": [],
   "source": [
    "# Algunos procedimientos y funciones auxiliares\n",
    "def imprimir_tweet_polaridad(tweet):\n",
    "  print(\"TWEET: \"+ tweet[1])\n",
    "  print(\"POLARIDAD: \"+ tweet[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "T2kOeMA_ly_b",
    "outputId": "eb2999c8-63db-4c94-fc79-18aaefbab705"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TWEET: Sssh, creo que estoy viendo a los Reyes... Pero, que hacen? Llevan palanquetas y los sacos vacíos, espera! Están llevándose la cuberteria! \n",
      "POLARIDAD: NONE\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import random\n",
    "\n",
    "# Preprocesamiento de los tweets\n",
    "\n",
    "# Definición de las sustituciones a realizar\n",
    "expresiones = [[r'#\\w+','HASHTAG'],\n",
    "                  [r'http\\S+', 'URL'], ['\\?\\?*', '?'],\n",
    "                  [r'\\!\\!*', '!'],['@\\w+', 'USUARIO'],\n",
    "                  [r'\\?', '? '],\n",
    "                  ['!', '! '],\n",
    "                  [',', ', '],\n",
    "                  [' ,', ','],\n",
    "                  [r'(?<!\\.)\\.(?!\\.)', '. '],\n",
    "                  [' q ', ' que '],\n",
    "                  [' ke ', ' que '],\n",
    "                  [' Q ', ' Que '],\n",
    "                  [' d ', ' de '],\n",
    "                  [r' (xq|pq) ', ' porque '],\n",
    "                  [' XQ ', ' porque '],\n",
    "                  [' m ', ' me '],\n",
    "                  [' m ', ' me '],\n",
    "                  ['M ', 'Me '],\n",
    "                  [' x ', ' por '],\n",
    "                  [r' (porfis|porfa|xfa) ', ' por favor '],\n",
    "                  [r'\\b(?:a*ja+j[aj]*)+\\b', 'jaja'],\n",
    "                  [r'\\b(?:a*ha+h[ah]*)+\\b', 'jaja'],\n",
    "                  [r'(?i) ud(\\. | )', ' Usted '],\n",
    "                  [r'(?i) uds(\\. | )', ' Ustedes '],\n",
    "                  ['\\.(\\.)+', '...'],\n",
    "                  [r'\\s(\\s)+', ' '],\n",
    "                  [':', ': ']\n",
    "                  ]\n",
    "\n",
    "# Se aplican todas las sustituciones definidas en \"expresiones\"\n",
    "def preprocesar_texto(texto):\n",
    "    for norm in expresiones:\n",
    "        texto = re.sub(str(norm[0]), str(norm[1]), texto)\n",
    "    return texto\n",
    "\n",
    "def preproc_dataset(dataset):\n",
    "  res = []\n",
    "  for twit in dataset:\n",
    "    twit_preproc = preprocesar_texto(twit[1])\n",
    "    res.append([twit[0], twit_preproc, twit[2]])\n",
    "  return res\n",
    "\n",
    "# Variables globales para los conjuntos preprocesados\n",
    "train_set_preproc = preproc_dataset(train_set)\n",
    "devel_set_preproc = preproc_dataset(devel_set)\n",
    "test_set_preproc = preproc_dataset(test_set)\n",
    "\n",
    "# Un ejemplo de train_set_preproc para corroborar que esté andando\n",
    "imprimir_tweet_polaridad(random.choice(train_set_preproc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YZMlbsw2uFLm"
   },
   "source": [
    "## Parte 2 - Representación de los tweets\n",
    "\n",
    "Para representar los tweets se pide que experimenten con modelos basados en Bag of Words (BoW) y con Word Embeddings.\n",
    "\n",
    "Para los dos enfoques podrán elegir entre diferentes opciones:\n",
    "\n",
    "**Bag of Words**\n",
    "\n",
    "* BOW estándar: se recomienda trabajar con la clase [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) de sklearn, en particular, fit_transform y transform.\n",
    "* BOW filtrando stop-words: tienen disponible en eva una lista de stop-words para el español, adaptada para análisis de sentimiento (no se filtran palabras relevantes para determinar la polaridad, como \"no\", \"pero\", etc.).\n",
    "* BoW usando lemas: pueden usar herramientas de spacy.\n",
    "* BOW seleccionando las features más relevantes: se recomienda usar la clase [SelectKBest](https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html?highlight=select%20k%20best#sklearn.feature_selection.SelectKBest) y probar con diferentes valores de k (por ejemplo, 10, 50, 200, 1000).\n",
    "* BOW combinado con TF-IDF: se recomienda usar la clase [TfidfVectorizer](https://https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)\n",
    "\n",
    "**Word Embeddings**\n",
    "\n",
    "* A partir de los word embeddings, representar cada tweet como el vector promedio (mean vector) de los vectores de las palabras que lo componen.\n",
    "* A partir de los word embeddings, representar cada tweet como la concatenación de los vectores de las palabras que lo componen (llevando el vector total a un largo fijo).\n",
    "\n",
    "Se recomienda trabajar con alguna de las colecciones de word embeddings disponibles en https://github.com/dccuchile/spanish-word-embeddings. El repositorio incluye links a ejemplos y tutoriales.\n",
    "\n",
    "\n",
    "Se pide que prueben al menos una opción basada en BoW y una basada en word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "Yy2afQwMX2r0"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from gensim.models.keyedvectors import KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G3tFcmytep5H"
   },
   "source": [
    "### Representación BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "Eiyoa6FoBoIN"
   },
   "outputs": [],
   "source": [
    "def tweets_from_corpus(corpus):\n",
    "     return [tuits[1] for tuits in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1ySO9kee6dG"
   },
   "source": [
    "#### BoW Estándar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lJJieI-p07jE"
   },
   "outputs": [],
   "source": [
    "def representar_en_bow(frase, vectorizer):\n",
    "    return vectorizer.transform([frase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "G9BNltLduHqB"
   },
   "outputs": [],
   "source": [
    "def bow_standard(corpus):\n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGBfezmce_gV"
   },
   "source": [
    "#### BOW filtrando stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "KCZtef8UfLy9"
   },
   "outputs": [],
   "source": [
    "def corpus_to_bow_stopwords(corpus):\n",
    "    vectorizer = CountVectorizer(stop_words=stop_words_esp_anasent)\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer , X.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4hTwjI1ZfETc"
   },
   "source": [
    "#### BoW combinado con TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0Bo2hGfeuP8"
   },
   "source": [
    "### Representación Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def corpus_to_bow_tf_idf(corpus):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    return vectorizer, X.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "K_T2KWO4RFIV"
   },
   "outputs": [],
   "source": [
    "# Representación de los tweets usando word embeddings\n",
    "wordvectors_file_vec = 'embeddings-l-model.vec'\n",
    "cantidad = 100000\n",
    "wordvectors = KeyedVectors.load_word2vec_format(wordvectors_file_vec, limit=cantidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CN9OuxV7fixy"
   },
   "source": [
    "#### WE Mean Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "3xNsCI7dXTRe"
   },
   "outputs": [],
   "source": [
    "# Representación de los tweets usando word embeddings representando cada tweet como el vector promedio (mean vector) de los vectores de las palabras que lo componen.\n",
    "def mean_vector(tweet,sin_stop_words, pre_procesado, largo_vector):\n",
    "    words_vectors = []\n",
    "    for word in tweet.split():\n",
    "        if sin_stop_words and word in stop_words_esp_anasent:\n",
    "            continue\n",
    "\n",
    "        if pre_procesado:\n",
    "            word = preprocesar_texto(word)\n",
    "\n",
    "        if word in wordvectors.key_to_index.keys():\n",
    "            words_vectors.append(wordvectors[word])\n",
    "\n",
    "    if len(words_vectors) == 0:\n",
    "        mean_vector = np.zeros(largo_vector)\n",
    "    else:\n",
    "        mean_vector = np.mean(words_vectors, axis=0)[0:largo_vector]\n",
    "    return mean_vector\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hg-sOzEHfobd"
   },
   "source": [
    "#### WE Concatenación de Vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "ZFyYXHmUWHr2"
   },
   "outputs": [],
   "source": [
    "def representar_tuit_concatvec_we(tuit, tamanio_rep):\n",
    "    palabras = tuit.split()\n",
    "    rep = []\n",
    "    for word in palabras:\n",
    "        # Hay que ver que hacer con las palabras que no tienen representación, las estamos ignorando\n",
    "        # pero se puede asignarle a las ignoradas un vector fijo por ej [0,0,0,...,0]\n",
    "        if word in wordvectors:\n",
    "            word_vector = wordvectors[word][:tamanio_rep]\n",
    "        else:\n",
    "            word_vector = np.zeros(tamanio_rep)\n",
    "        rep.extend(word_vector)\n",
    "    return rep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pxAcPhqgWHr2"
   },
   "source": [
    "## Parte 3 - Clasificación de los tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "W9FFTkuWWHr3"
   },
   "outputs": [],
   "source": [
    "# imports\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from pysentimiento import create_analyzer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Variables globales para los clasificadores\n",
    "plt.style.use('fivethirtyeight')\n",
    "sentimiento_map = {\"NEU\": \"NONE\", \"NEG\": \"N\", \"POS\": \"P\"}\n",
    "tamanio = 100\n",
    "cantidad_entradas_vec_tuit = 300\n",
    "tamanio_vec_concat_we = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "mRXkXjwJqkEs"
   },
   "outputs": [],
   "source": [
    "# Funciones auxiliares para generar los array de datos junto con sus polaridades para cada enfoque\n",
    "def generar_datos_polaridad_concatvec_we(tamanio_rep_palabras, cantidad_palabras_tuit, corpus):\n",
    "    polaridad = []\n",
    "    datos = []\n",
    "\n",
    "    for entrada in  corpus:\n",
    "        vec_entrada = representar_tuit_concatvec_we(entrada[1].lower(),tamanio_rep_palabras)\n",
    "        #Hago esto por si se pasa de largo\n",
    "        vec_entrada = vec_entrada[0:min(len(vec_entrada), cantidad_palabras_tuit)]\n",
    "        datos.append(vec_entrada)\n",
    "\n",
    "        polaridad.append(entrada[2])\n",
    "\n",
    "    datos_padded = [np.pad(vec, (0, cantidad_palabras_tuit - len(vec)), mode='constant') for vec in datos]\n",
    "\n",
    "    datos = np.vstack(datos_padded)\n",
    "    polaridad = np.array(polaridad)\n",
    "    return datos, polaridad\n",
    "\n",
    "def generar_datos_polaridad_mean_vector_we(cantidad_entradas_vec, sin_stop_words, pre_procesado, corpus):\n",
    "    polaridad = []\n",
    "    datos = []\n",
    "\n",
    "    for entrada in corpus:\n",
    "        vec_entrada = mean_vector(entrada[1].lower(), sin_stop_words, pre_procesado, cantidad_entradas_vec)\n",
    "\n",
    "        vec_entrada = np.array(vec_entrada)\n",
    "\n",
    "        vec_entrada = vec_entrada[0:cantidad_entradas_vec]\n",
    "        polaridad.append(entrada[2])\n",
    "        datos.append(vec_entrada)\n",
    "\n",
    "    polaridad = np.array(polaridad)\n",
    "    return datos, polaridad\n",
    "\n",
    "def generar_datos_polaridad_bow_stopwords(corpus, vectorizer):\n",
    "    polaridad = []\n",
    "    datos = []\n",
    "    for entrada in corpus:\n",
    "        vec_entrada = representar_en_bow(entrada[1].lower(), vectorizer)\n",
    "        vec_entrada = vec_entrada.toarray()\n",
    "        polaridad.append(entrada[2])\n",
    "        datos.append(vec_entrada)\n",
    "\n",
    "    datos = np.concatenate(datos, axis=0)\n",
    "    polaridad = np.array(polaridad)\n",
    "    return datos, polaridad\n",
    "\n",
    "def generar_array_polaridad(corpus):\n",
    "    return [x[2] for x in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xtq7Fy-LOhgT"
   },
   "source": [
    "#### Experimentos MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wCEupw52OyGr"
   },
   "source": [
    "##### MLP utilizando BoW stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "Ii0VqnkOO3x8"
   },
   "outputs": [],
   "source": [
    "def entrenar_mlp_bow_stopwords(corpus, vectorizer):\n",
    "    datos, polaridad = generar_datos_polaridad_bow_stopwords(corpus, vectorizer)\n",
    "    clf = MLPClassifier(random_state=1, max_iter=300)\n",
    "    clf.fit(datos, polaridad)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "UitZBpL_vnYZ"
   },
   "outputs": [],
   "source": [
    "# Test MLP BoW Stopwords\n",
    "\n",
    "def macroF1_bow(clf, vectorizer, corpus):\n",
    "    datos, y_real = generar_datos_polaridad_bow_stopwords(corpus, vectorizer)\n",
    "    y_pred = [clf.predict(representacion_tuit.reshape(1, -1)) for representacion_tuit in datos]\n",
    "\n",
    "    return f1_score(y_real, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oXSZAfUayDBw"
   },
   "source": [
    "##### MLP utilizando Mean Vector (WE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "6JOo9d1Eq2LJ"
   },
   "outputs": [],
   "source": [
    "def entrenar_mlp_mean_vector(cantidad_entradas_vec, sin_stop_words, pre_procesado, corpus):\n",
    "  clf = MLPClassifier(random_state=1, max_iter=30000)\n",
    "  datos, polaridad = generar_datos_polaridad_mean_vector_we(cantidad_entradas_vec,sin_stop_words, pre_procesado, corpus)\n",
    "  clf.fit(datos, polaridad)\n",
    "\n",
    "  return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7R0rlZMhednn"
   },
   "source": [
    "### Experimentos SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fYJ3fIPOPq4"
   },
   "source": [
    "#### SVM utilizando WE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_PXrqfFcIGr"
   },
   "source": [
    "##### SVM con concatenación de vectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "ZTPJtTCVuxj8"
   },
   "outputs": [],
   "source": [
    "def entrenar_svm_concatvec(tamanio_rep_palabras, cantidad_palabras_tuit,  corpus):\n",
    "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "    datos, polaridad = generar_datos_polaridad_concatvec_we(tamanio_rep_palabras, cantidad_palabras_tuit, corpus)\n",
    "    clf.fit(datos, polaridad)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "UvKtcEQBqv96"
   },
   "outputs": [],
   "source": [
    "# Test SVM con concatvec\n",
    "def predecir_polaridad_concat(tuit, cantidad_entradas_vec, clf):\n",
    "    if cantidad_entradas_vec - len(tuit) < 0:\n",
    "        tuit = tuit[0:cantidad_entradas_vec]\n",
    "    else:\n",
    "        tuit = np.pad(\n",
    "            tuit,\n",
    "            (0, cantidad_entradas_vec - len(tuit)),\n",
    "            mode=\"constant\",\n",
    "        )\n",
    "    return clf.predict([tuit])\n",
    "\n",
    "def macro_F1_concatvec_we(tamanio_rep_palabras, cantidad_entradas_vec, clf, corpus):\n",
    "    datos, y_real = generar_datos_polaridad_concatvec_we(tamanio_rep_palabras, cantidad_entradas_vec, corpus)\n",
    "    y_pred = [predecir_polaridad_concat(x, cantidad_entradas_vec, clf) for x in datos]\n",
    "    return f1_score(y_real, y_pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LuML6RMLcSot"
   },
   "source": [
    "##### SVM con Mean Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "9-NMtzzRrFvx"
   },
   "outputs": [],
   "source": [
    "# SVM Mean Vector\n",
    "def entrenar_svm_mean_vector(cantidad_entradas_vec,sin_stop_words, pre_procesado, corpus):\n",
    "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "    datos, polaridad = generar_datos_polaridad_mean_vector_we(cantidad_entradas_vec,sin_stop_words, pre_procesado, corpus)\n",
    "    clf.fit(datos, polaridad)\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "id": "SL3BmrS8smjg"
   },
   "outputs": [],
   "source": [
    "def predecir_polaridad_mean_vector(tuit, cantidad_entradas_vec, clf):\n",
    "    if cantidad_entradas_vec - len(tuit) < 0:\n",
    "        representacion_tuit = tuit[0:cantidad_entradas_vec]\n",
    "    else:\n",
    "        representacion_tuit = np.pad(\n",
    "           tuit,\n",
    "            (0, cantidad_entradas_vec - len(tuit)),\n",
    "            mode=\"constant\",\n",
    "        )\n",
    "    return clf.predict([representacion_tuit])\n",
    "\n",
    "def macro_F1_mean_vector(cantidad_entradas_vec,sin_stop_words, pre_procesado, clf, corpus):\n",
    "    datos, y_real = generar_datos_polaridad_mean_vector_we(cantidad_entradas_vec, sin_stop_words, pre_procesado, corpus)\n",
    "    y_pred = [predecir_polaridad_mean_vector(x, cantidad_entradas_vec, clf) for x in datos]\n",
    "    return f1_score(y_real, y_pred, average=\"macro\")\n",
    "\n",
    "def accuracy_score_mean_vector(cantidad_entradas_vec,sin_stop_words, pre_procesado, clf, corpus):\n",
    "    datos, y_real = generar_datos_polaridad_mean_vector_we(cantidad_entradas_vec, sin_stop_words, pre_procesado, corpus)\n",
    "    y_pred = [predecir_polaridad_mean_vector(x, cantidad_entradas_vec, clf) for x in datos]\n",
    "    return accuracy_score(y_real, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKy9LcHpvJKL"
   },
   "source": [
    "### Ensamble Voting de SVM y MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "A21a3nTXrC7J"
   },
   "outputs": [],
   "source": [
    "# Ensamble Voting de SVM y MLP\n",
    "def entrenar_ensamblado_de_votacion_svm_mlp(cantidad_entradas_vec, sin_stop_words, pre_procesado, corpus):\n",
    "  clf_svm = entrenar_svm_mean_vector(cantidad_entradas_vec,sin_stop_words, pre_procesado, corpus)\n",
    "  clf_mlp = entrenar_mlp_mean_vector(cantidad_entradas_vec,sin_stop_words, pre_procesado, corpus)\n",
    "  datos, polaridad = generar_datos_polaridad_mean_vector_we(cantidad_entradas_vec,sin_stop_words, pre_procesado, corpus)\n",
    "\n",
    "  clf_votacion = VotingClassifier(estimators=[('svm', clf_svm), ('mlp', clf_mlp)], voting='hard')\n",
    "  clf_votacion.fit(datos, polaridad)\n",
    "\n",
    "  return clf_votacion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OTQwj7uZcxXg"
   },
   "source": [
    "### Resultados de los experimentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MLP con BoW Entrenamiento\n",
    "\n",
    "vectorizer_bow_sw, x_sw = corpus_to_bow_stopwords(tweets_from_corpus(train_set))\n",
    "clf_mlp_bow_stopwords = entrenar_mlp_bow_stopwords(train_set, vectorizer_bow_sw)\n",
    "\n",
    "vectorizer_bow_tf_idf, x_tf_idf = corpus_to_bow_tf_idf(tweets_from_corpus(train_set))\n",
    "clf_mlp_bow_tf_idf = entrenar_mlp_bow_stopwords(train_set, vectorizer_bow_tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP utilizando BoW\n",
      "╒════════════╤════════════╕\n",
      "│            │   Macro F1 │\n",
      "╞════════════╪════════════╡\n",
      "│ BoW sin SW │    0.55153 │\n",
      "├────────────┼────────────┤\n",
      "│ BoW TF-IDF │    0.55004 │\n",
      "╘════════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "# MLP con BoW\n",
    "res = []\n",
    "headers = ['', 'Macro F1']\n",
    "\n",
    "macroF1_bow_sw =  macroF1_bow(clf_mlp_bow_stopwords, vectorizer_bow_sw, devel_set)\n",
    "res.append([\"BoW sin SW\", macroF1_bow_sw])\n",
    "\n",
    "macroF1_bow_tf_idf = macroF1_bow(clf_mlp_bow_tf_idf, vectorizer_bow_tf_idf, devel_set)\n",
    "res.append([\"BoW TF-IDF\", macroF1_bow_tf_idf])\n",
    "\n",
    "print(\"MLP utilizando BoW\")\n",
    "print(tabulate(res, headers=headers, tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "entrenamiento_agrandado = np.concatenate((np.array(train_set),np.array(devel_set)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Entrenamiento MLP Mean Vector\n",
    "\n",
    "clf_mlp_mean_vector_sin_stop_words_preproc = entrenar_mlp_mean_vector(cantidad_entradas_vec_tuit, True, True, train_set)\n",
    "clf_mlp_mean_vector_sin_stop_words = entrenar_mlp_mean_vector(cantidad_entradas_vec_tuit ,True,False,train_set)\n",
    "clf_mlp_mean_vector_preproc = entrenar_mlp_mean_vector(cantidad_entradas_vec_tuit,False,True, train_set)\n",
    "clf_mlp_mean_vector = entrenar_mlp_mean_vector(cantidad_entradas_vec_tuit,False,False, train_set)\n",
    "\n",
    "clf_mlp_mean_vector_sin_stop_words_preproc_agrandado = entrenar_mlp_mean_vector(cantidad_entradas_vec_tuit, True, True, entrenamiento_agrandado)\n",
    "clf_mlp_mean_vector_sin_stop_words_agrandado = entrenar_mlp_mean_vector(cantidad_entradas_vec_tuit ,True,False,entrenamiento_agrandado)\n",
    "clf_mlp_mean_vector_preproc_agrandado = entrenar_mlp_mean_vector(cantidad_entradas_vec_tuit,False,True, entrenamiento_agrandado)\n",
    "clf_mlp_mean_vector_agrandado = entrenar_mlp_mean_vector(cantidad_entradas_vec_tuit,False,False, entrenamiento_agrandado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "EqYPHb9Icz09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP usando de entrenamiento train_set\n",
      "╒══════════╤═════════════════╤════════════╕\n",
      "│ Sin SW   │ Pre-procesado   │   Macro F1 │\n",
      "╞══════════╪═════════════════╪════════════╡\n",
      "│ Sí       │ Sí              │   0.563197 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ Sí       │ No              │   0.565241 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ No       │ Sí              │   0.560286 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ No       │ No              │   0.552427 │\n",
      "╘══════════╧═════════════════╧════════════╛\n",
      "MLP usando de entrenamiento train_set+devel_set (probado en test_set)\n",
      "╒══════════╤═════════════════╤════════════╕\n",
      "│ Sin SW   │ Pre-procesado   │   Macro F1 │\n",
      "╞══════════╪═════════════════╪════════════╡\n",
      "│ Sí       │ Sí              │   0.543757 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ Sí       │ No              │   0.541133 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ No       │ Sí              │   0.55644  │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ No       │ No              │   0.546341 │\n",
      "╘══════════╧═════════════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "# MLP con mean vector\n",
    "res = []\n",
    "headers = ['Sin SW', 'Pre-procesado', 'Macro F1']\n",
    "\n",
    "macro_f1_mlp_mean_vector_sin_stop_words_preproc = macro_F1_mean_vector(cantidad_entradas_vec_tuit, True, True, clf_mlp_mean_vector_sin_stop_words_preproc, devel_set)\n",
    "res.append([\"Sí\",\"Sí\", macro_f1_mlp_mean_vector_sin_stop_words_preproc])\n",
    "\n",
    "macro_f1_mlp_mean_vector_sin_stop_words = macro_F1_mean_vector(cantidad_entradas_vec_tuit,True,False, clf_mlp_mean_vector_sin_stop_words, devel_set)\n",
    "res.append([\"Sí\", \"No\", macro_f1_mlp_mean_vector_sin_stop_words])\n",
    "\n",
    "macro_f1_mlp_mean_vector_pre_proc =  macro_F1_mean_vector(cantidad_entradas_vec_tuit,False,True, clf_mlp_mean_vector_preproc, devel_set)\n",
    "res.append([\"No\", \"Sí\", macro_f1_mlp_mean_vector_pre_proc])\n",
    "\n",
    "macro_f1_mlp_mean_vector = macro_F1_mean_vector(cantidad_entradas_vec_tuit,False,False,clf_mlp_mean_vector, devel_set)\n",
    "res.append([\"No\", \"No\", macro_f1_mlp_mean_vector])\n",
    "\n",
    "print(\"MLP usando de entrenamiento train_set\")\n",
    "print(tabulate(res, headers=headers, tablefmt='fancy_grid'))\n",
    "\n",
    "# MLP Mean Vector agrandando el conjunto de entrenamiento\n",
    "res = []\n",
    "\n",
    "macro_f1_mlp_mean_vector_sin_stop_words_pre_proc_agrandado = macro_F1_mean_vector(cantidad_entradas_vec_tuit, True, True, clf_mlp_mean_vector_sin_stop_words_preproc_agrandado, test_set)\n",
    "res.append([\"Sí\",\"Sí\", macro_f1_mlp_mean_vector_sin_stop_words_pre_proc_agrandado])\n",
    "\n",
    "macro_f1_mlp_mean_vector_sin_stop_words_agrandado = macro_F1_mean_vector(cantidad_entradas_vec_tuit,True,False, clf_mlp_mean_vector_sin_stop_words_agrandado, test_set)\n",
    "res.append([\"Sí\", \"No\", macro_f1_mlp_mean_vector_sin_stop_words_agrandado])\n",
    "\n",
    "macro_f1_mlp_mean_vector_pre_proc_agrandado =  macro_F1_mean_vector(cantidad_entradas_vec_tuit,False,True, clf_mlp_mean_vector_preproc_agrandado, test_set)\n",
    "res.append([\"No\", \"Sí\", macro_f1_mlp_mean_vector_pre_proc_agrandado])\n",
    "\n",
    "macro_f1_mlp_mean_vector_agrandado = macro_F1_mean_vector(cantidad_entradas_vec_tuit,False,False,clf_mlp_mean_vector_agrandado, test_set)\n",
    "res.append([\"No\", \"No\", macro_f1_mlp_mean_vector_agrandado])\n",
    "\n",
    "print(\"MLP usando de entrenamiento train_set+devel_set (probado en test_set)\")\n",
    "print(tabulate(res, headers=headers, tablefmt='fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "T7kMhdNnc2Gp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score SVM concatenacion de vectores:  0.4121280805187426\n"
     ]
    }
   ],
   "source": [
    "# SVM concatenacion de vectores\n",
    "clf_svm_concatvec = entrenar_svm_concatvec(tamanio,tamanio_vec_concat_we, train_set)\n",
    "print(\"F1 Score SVM concatenacion de vectores: \", macro_F1_concatvec_we(tamanio, tamanio_vec_concat_we, clf_svm_concatvec, devel_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SVM Mean Vector Entrenamiento\n",
    "\n",
    "clf_svm_mean_vector_sin_stop_words_preproc = entrenar_svm_mean_vector(cantidad_entradas_vec_tuit,True,True, train_set)\n",
    "clf_svm_mean_vector_sin_stop_words = entrenar_svm_mean_vector(cantidad_entradas_vec_tuit,True,False, train_set)\n",
    "clf_svm_mean_vector_preproc = entrenar_svm_mean_vector(cantidad_entradas_vec_tuit,False,True, train_set)\n",
    "clf_svm_mean_vector = entrenar_svm_mean_vector(cantidad_entradas_vec_tuit,False,False, train_set)\n",
    "\n",
    "clf_svm_mean_vector_sin_stop_words_preproc_agrandado = entrenar_svm_mean_vector(cantidad_entradas_vec_tuit,True,True, entrenamiento_agrandado)\n",
    "clf_svm_mean_vector_sin_stop_words_agrandado = entrenar_svm_mean_vector(cantidad_entradas_vec_tuit,True,False, entrenamiento_agrandado)\n",
    "clf_svm_mean_vector_preproc_agrandado = entrenar_svm_mean_vector(cantidad_entradas_vec_tuit,False,True, entrenamiento_agrandado)\n",
    "clf_svm_mean_vector_agrandado = entrenar_svm_mean_vector(cantidad_entradas_vec_tuit,False,False, entrenamiento_agrandado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "id": "lw7Qw5mic34T"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruebas con SVM Mean Vector entrenando en train_set\n",
      "╒══════════╤═════════════════╤════════════╕\n",
      "│ Sin SW   │ Pre-procesado   │   Macro F1 │\n",
      "╞══════════╪═════════════════╪════════════╡\n",
      "│ Sí       │ Sí              │   0.615335 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ Sí       │ No              │   0.607947 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ No       │ Sí              │   0.589528 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ No       │ No              │   0.588391 │\n",
      "╘══════════╧═════════════════╧════════════╛\n",
      "Pruebas con SVM Mean Vector entrenando en train_set+devel_set (probado en test_set)\n",
      "╒══════════╤═════════════════╤════════════╕\n",
      "│ Sin SW   │ Pre-procesado   │   Macro F1 │\n",
      "╞══════════╪═════════════════╪════════════╡\n",
      "│ Sí       │ Sí              │   0.604831 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ Sí       │ No              │   0.60103  │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ No       │ Sí              │   0.610811 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ No       │ No              │   0.606212 │\n",
      "╘══════════╧═════════════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "# SVM Mean Vector\n",
    "headers = ['Sin SW', 'Pre-procesado', 'Macro F1']\n",
    "res = []\n",
    "\n",
    "macro_F1_svm_mean_vector_sin_stop_words_pre_proc = macro_F1_mean_vector(cantidad_entradas_vec_tuit,True,True, clf_svm_mean_vector_sin_stop_words_preproc, devel_set)\n",
    "res.append([\"Sí\",\"Sí\", macro_F1_svm_mean_vector_sin_stop_words_pre_proc])\n",
    "\n",
    "macro_F1_svm_mean_vector_sin_stop_words = macro_F1_mean_vector(cantidad_entradas_vec_tuit,True,False, clf_svm_mean_vector_sin_stop_words, devel_set)\n",
    "res.append([\"Sí\",\"No\", macro_F1_svm_mean_vector_sin_stop_words])\n",
    "\n",
    "macro_F1_svm_mean_vector_sin_preproc = macro_F1_mean_vector(cantidad_entradas_vec_tuit,False,True, clf_svm_mean_vector_preproc, devel_set)\n",
    "res.append([\"No\",\"Sí\", macro_F1_svm_mean_vector_sin_preproc])\n",
    "\n",
    "macro_F1_svm_mean_vector = macro_F1_mean_vector(cantidad_entradas_vec_tuit,False,False,clf_svm_mean_vector, devel_set)\n",
    "res.append([\"No\",\"No\", macro_F1_svm_mean_vector])\n",
    "\n",
    "print(\"Pruebas con SVM Mean Vector entrenando en train_set\")\n",
    "print(tabulate(res, headers=headers, tablefmt='fancy_grid'))\n",
    "\n",
    "# SVM Mean Vector agrandando el conjunto de entrenamiento\n",
    "res = []\n",
    "\n",
    "macro_F1_svm_mean_vector_sin_stop_words_pre_proc_agrandado = macro_F1_mean_vector(cantidad_entradas_vec_tuit,True,True, clf_svm_mean_vector_sin_stop_words_preproc_agrandado, test_set)\n",
    "res.append([\"Sí\",\"Sí\", macro_F1_svm_mean_vector_sin_stop_words_pre_proc_agrandado])\n",
    "\n",
    "macro_F1_svm_mean_vector_sin_stop_words_agrandado = macro_F1_mean_vector(cantidad_entradas_vec_tuit,True,False, clf_svm_mean_vector_sin_stop_words_agrandado, test_set)\n",
    "res.append([\"Sí\",\"No\", macro_F1_svm_mean_vector_sin_stop_words_agrandado])\n",
    "\n",
    "macro_F1_svm_mean_vector_sin_pre_proc_agrandado = macro_F1_mean_vector(cantidad_entradas_vec_tuit,False,True, clf_svm_mean_vector_preproc_agrandado, test_set)\n",
    "res.append([\"No\",\"Sí\", macro_F1_svm_mean_vector_sin_pre_proc_agrandado])\n",
    "\n",
    "macro_F1_svm_mean_vector_agrandado = macro_F1_mean_vector(cantidad_entradas_vec_tuit,False,False,clf_svm_mean_vector_agrandado, test_set)\n",
    "res.append([\"No\",\"No\", macro_F1_svm_mean_vector_agrandado])\n",
    "\n",
    "print(\"Pruebas con SVM Mean Vector entrenando en train_set+devel_set (probado en test_set)\")\n",
    "print(tabulate(res, headers=headers, tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Ensamblado de votación con SVM y MLP Entrenamiento\n",
    "clf_ensamblado_votacion_svm_mlp_sin_stop_words_preproc = entrenar_ensamblado_de_votacion_svm_mlp(cantidad_entradas_vec_tuit, True, True, train_set)\n",
    "clf_ensamblado_votacion_svm_mlp_sin_stop_words = entrenar_ensamblado_de_votacion_svm_mlp(cantidad_entradas_vec_tuit, True, False, train_set)\n",
    "clf_ensamblado_votacion_svm_mlp_preproc = entrenar_ensamblado_de_votacion_svm_mlp(cantidad_entradas_vec_tuit, False, True, train_set)\n",
    "clf_ensamblado_votacion_svm_mlp = entrenar_ensamblado_de_votacion_svm_mlp(cantidad_entradas_vec_tuit, False, False, train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensamblado de votación entre SVM y MLP\n",
      "╒══════════╤═════════════════╤════════════╕\n",
      "│ Sin SW   │ Pre-procesado   │   Macro F1 │\n",
      "╞══════════╪═════════════════╪════════════╡\n",
      "│ Sí       │ Sí              │   0.585081 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ Sí       │ No              │   0.5879   │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ No       │ Sí              │   0.585909 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ No       │ No              │   0.585845 │\n",
      "╘══════════╧═════════════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Ensamblado de votación con SVM y MLP\n",
    "headers = ['Sin SW', 'Pre-procesado', 'Macro F1']\n",
    "res = []\n",
    "\n",
    "macro_F1_ensamblado_votacion_svm_mlp_sin_stop_words_preproc = macro_F1_mean_vector(cantidad_entradas_vec_tuit,True,True, clf_ensamblado_votacion_svm_mlp_sin_stop_words_preproc, test_set)\n",
    "res.append([\"Sí\",\"Sí\", macro_F1_ensamblado_votacion_svm_mlp_sin_stop_words_preproc])\n",
    "\n",
    "macro_F1_ensamblado_votacion_svm_mlp_sin_stop_words = macro_F1_mean_vector(cantidad_entradas_vec_tuit,True,False, clf_ensamblado_votacion_svm_mlp_sin_stop_words, test_set)\n",
    "res.append([\"Sí\",\"No\", macro_F1_ensamblado_votacion_svm_mlp_sin_stop_words])\n",
    "\n",
    "macro_F1_ensamblado_votacion_svm_mlp_preproc  = macro_F1_mean_vector(cantidad_entradas_vec_tuit,False,True, clf_ensamblado_votacion_svm_mlp_preproc, test_set)\n",
    "res.append([\"No\",\"Sí\", macro_F1_ensamblado_votacion_svm_mlp_preproc])\n",
    "\n",
    "macro_F1_ensamblado_votacion_svm_mlp  = macro_F1_mean_vector(cantidad_entradas_vec_tuit,False,False,clf_ensamblado_votacion_svm_mlp, test_set)\n",
    "res.append([\"No\",\"No\", macro_F1_ensamblado_votacion_svm_mlp])\n",
    "\n",
    "print(\"Ensamblado de votación entre SVM y MLP\")\n",
    "print(tabulate(res, headers=headers, tablefmt='fancy_grid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abBp1Qmsdnsq"
   },
   "source": [
    "### Hiperparámetros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "F8y4fTJbdqPs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando mejor paramétro entre:  1 0.01\n",
      "El mejor clasificador es:  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(C=1, gamma='auto'))])\n",
      "Buscando mejor paramétro entre:  1 0.505\n",
      "El mejor clasificador es:  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(C=1, gamma='auto'))])\n",
      "Buscando mejor paramétro entre:  1 0.7525\n",
      "El mejor clasificador es:  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(C=0.7525, gamma='auto'))])\n",
      "Buscando mejor paramétro entre:  0.7525 0.87625\n",
      "El mejor clasificador es:  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(C=0.7525, gamma='auto'))])\n",
      "Buscando mejor paramétro entre:  0.7525 0.814375\n",
      "El mejor clasificador es:  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(C=0.814375, gamma='auto'))])\n",
      "Buscando mejor paramétro entre:  0.814375 0.7834375\n",
      "El mejor clasificador es:  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(C=0.814375, gamma='auto'))])\n",
      "Buscando mejor paramétro entre:  0.814375 0.7989062499999999\n",
      "El mejor clasificador es:  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(C=0.814375, gamma='auto'))])\n",
      "Buscando mejor paramétro entre:  0.814375 0.806640625\n",
      "El mejor clasificador es:  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(C=0.806640625, gamma='auto'))])\n",
      "Buscando mejor paramétro entre:  0.806640625 0.8105078125\n",
      "El mejor clasificador es:  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(C=0.8105078125, gamma='auto'))])\n",
      "Buscando mejor paramétro entre:  0.8105078125 0.80857421875\n",
      "El mejor clasificador es:  Pipeline(steps=[('standardscaler', StandardScaler()),\n",
      "                ('svc', SVC(C=0.80857421875, gamma='auto'))])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "def encontrar_mejor_hiperparametros(cantidad_entradas_vec, corpus, parametros):\n",
    "    parametros_busqueda = {\n",
    "        'svc__C': parametros,\n",
    "        'svc__kernel': ['linear', 'rbf', 'sigmoid']\n",
    "    }\n",
    "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "    grid_search = GridSearchCV(clf, parametros_busqueda)\n",
    "    datos, polaridad = generar_datos_polaridad_mean_vector_we(cantidad_entradas_vec, True, False, corpus)\n",
    "    grid_search.fit(datos, polaridad)\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "def buscar_mejor_hiperparametros_en_intervalo(cantidad_entradas_vec, corpus, param_1, param_2, limite):\n",
    "    mejor_param = 0\n",
    "    mejor_clf = 0\n",
    "    for i in range(limite):\n",
    "        print(\"Buscando mejor paramétro entre: \", param_1, param_2)\n",
    "        mejor_clf = encontrar_mejor_hiperparametros(cantidad_entradas_vec, corpus, [param_1, param_2])\n",
    "        mejor_param = mejor_clf[1].C\n",
    "        print(\"El mejor clasificador es: \", mejor_clf)\n",
    "\n",
    "        if mejor_param == param_2:\n",
    "            peor_param = param_1\n",
    "        else:\n",
    "            peor_param = param_2\n",
    "\n",
    "        if mejor_param > peor_param:\n",
    "            medio = (mejor_param - peor_param)/2 + peor_param\n",
    "        else:\n",
    "            medio = (peor_param - mejor_param)/2 + mejor_param\n",
    "\n",
    "        param_1 = mejor_param\n",
    "        param_2 = medio\n",
    "\n",
    "\n",
    "    return mejor_param, mejor_clf\n",
    "\n",
    "mejor_c, mejor_modelo = buscar_mejor_hiperparametros_en_intervalo(256, train_set, 1,0.01, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "id": "SZSxQPH-dtmA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "El valor de C es 1.0 y el valor de gamma es auto\n",
      "Macro F1 probado en devel_set 0.6136053723342164\n"
     ]
    }
   ],
   "source": [
    "# Obtener los hiperparámetros utilizados\n",
    "svm = clf_svm_mean_vector.named_steps['svc']  # Obtener el estimador SVM del pipeline\n",
    "C = svm.C  # Obtener el valor de C\n",
    "gamma = svm.gamma  # Obtener el valor de gamma\n",
    "\n",
    "print(f\"El valor de C es {C} y el valor de gamma es {gamma}\")\n",
    "\n",
    "macro_f1_con_mejores_hiperparametros = macro_F1_mean_vector(256,True,True, mejor_modelo, devel_set)\n",
    "print(\"Macro F1 probado en devel_set\", macro_f1_con_mejores_hiperparametros)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e6VGFrEoWHr3"
   },
   "source": [
    "## Parte 4: Evaluación sobre test\n",
    "\n",
    "Deben probar los mejores modelos obtenidos en la parte anterior sobre el corpus de test.\n",
    "\n",
    "También deben comparar sus resultados con un modelo pre-entrenado para análisis de sentimientos de la biblioteca [pysentimiento](https://github.com/pysentimiento/pysentimiento) (deben aplicarlo sobre el corpus de test).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score pysentimiento 0.7041227233971533\n"
     ]
    }
   ],
   "source": [
    "# Pysentimiento\n",
    "def macro_F1_pysentimiento(corpus):\n",
    "    analyzer = create_analyzer(task=\"sentiment\", lang=\"es\")\n",
    "    y_pred = []\n",
    "    y_real = [elem[2] for elem in corpus]\n",
    "    for elemento in corpus:\n",
    "        analizer_result = analyzer.predict(elemento[1]).output\n",
    "        y_pred.append(sentimiento_map[analizer_result])\n",
    "    return f1_score(y_real, y_pred, average=\"macro\")\n",
    "\n",
    "print(\"F1 Score pysentimiento\", macro_F1_pysentimiento(test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruebas MLP sobre test_set\n",
      "╒══════════╤═════════════════╤════════════╕\n",
      "│ Sin SW   │ Pre-procesado   │   Macro F1 │\n",
      "╞══════════╪═════════════════╪════════════╡\n",
      "│ Sí       │ Sí              │   0.558917 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ Sí       │ No              │   0.56165  │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ No       │ Sí              │   0.556806 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ No       │ No              │   0.551947 │\n",
      "╘══════════╧═════════════════╧════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Evaluación sobre test MLP\n",
    "\n",
    "print(\"Pruebas MLP sobre test_set\")\n",
    "res = []\n",
    "headers = ['Sin SW', 'Pre-procesado', 'Macro F1']\n",
    "\n",
    "macro_f1_mlp_mean_vector_sin_stop_words_pre_proc = macro_F1_mean_vector(cantidad_entradas_vec_tuit, True, True, clf_mlp_mean_vector_sin_stop_words_preproc, test_set)\n",
    "res.append([\"Sí\",\"Sí\", macro_f1_mlp_mean_vector_sin_stop_words_pre_proc])\n",
    "\n",
    "macro_f1_mlp_mean_vector_sin_stop_words = macro_F1_mean_vector(cantidad_entradas_vec_tuit,True,False, clf_mlp_mean_vector_sin_stop_words, test_set)\n",
    "res.append([\"Sí\", \"No\", macro_f1_mlp_mean_vector_sin_stop_words])\n",
    "\n",
    "macro_f1_mlp_mean_vector_pre_proc =  macro_F1_mean_vector(cantidad_entradas_vec_tuit,False,True, clf_mlp_mean_vector_preproc, test_set)\n",
    "res.append([\"No\", \"Sí\", macro_f1_mlp_mean_vector_pre_proc])\n",
    "\n",
    "macro_f1_mlp_mean_vector = macro_F1_mean_vector(cantidad_entradas_vec_tuit,False,False,clf_mlp_mean_vector, test_set)\n",
    "res.append([\"No\", \"No\", macro_f1_mlp_mean_vector])\n",
    "\n",
    "print(tabulate(res, headers=headers, tablefmt='fancy_grid'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruebas SVM Mean Vector sobre test_set\n",
      "╒══════════╤═════════════════╤════════════╕\n",
      "│ Sin SW   │ Pre-procesado   │   Macro F1 │\n",
      "╞══════════╪═════════════════╪════════════╡\n",
      "│ Sí       │ Sí              │   0.607559 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ Sí       │ No              │   0.601875 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ No       │ Sí              │   0.609586 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ No       │ No              │   0.608642 │\n",
      "╘══════════╧═════════════════╧════════════╛\n",
      "Pruebas con Mean Vector entrenando en train_set+devel_set\n",
      "╒══════════╤═════════════════╤════════════╕\n",
      "│ Sin SW   │ Pre-procesado   │   Macro F1 │\n",
      "╞══════════╪═════════════════╪════════════╡\n",
      "│ Sí       │ Sí              │   0.604831 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ Sí       │ No              │   0.60103  │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ No       │ Sí              │   0.610811 │\n",
      "├──────────┼─────────────────┼────────────┤\n",
      "│ No       │ No              │   0.606212 │\n",
      "╘══════════╧═════════════════╧════════════╛\n",
      "Macro F1 probado en test_set con los hiperparámetros mejorados:  0.607014455759271\n"
     ]
    }
   ],
   "source": [
    "# Prueba SVM Mean Vector sobre test_set\n",
    "headers = ['Sin SW', 'Pre-procesado', 'Macro F1']\n",
    "res = []\n",
    "\n",
    "macro_F1_svm_mean_vector_sin_stop_words_preproc = macro_F1_mean_vector(cantidad_entradas_vec_tuit,True,True, clf_svm_mean_vector_sin_stop_words_preproc, test_set)\n",
    "res.append([\"Sí\",\"Sí\", macro_F1_svm_mean_vector_sin_stop_words_preproc])\n",
    "\n",
    "macro_F1_svm_mean_vector_sin_stop_words = macro_F1_mean_vector(cantidad_entradas_vec_tuit,True,False, clf_svm_mean_vector_sin_stop_words, test_set)\n",
    "res.append([\"Sí\",\"No\", macro_F1_svm_mean_vector_sin_stop_words])\n",
    "\n",
    "macro_F1_svm_mean_vector_sin_preproc = macro_F1_mean_vector(cantidad_entradas_vec_tuit,False,True, clf_svm_mean_vector_preproc, test_set)\n",
    "res.append([\"No\",\"Sí\", macro_F1_svm_mean_vector_sin_preproc])\n",
    "\n",
    "macro_F1_svm_mean_vector = macro_F1_mean_vector(cantidad_entradas_vec_tuit,False,False,clf_svm_mean_vector, test_set)\n",
    "res.append([\"No\",\"No\", macro_F1_svm_mean_vector])\n",
    "\n",
    "print(\"Pruebas SVM Mean Vector sobre test_set\")\n",
    "print(tabulate(res, headers=headers, tablefmt='fancy_grid'))\n",
    "\n",
    "# SVM Mean Vector agrandando el conjunto de entrenamiento\n",
    "res = []\n",
    "\n",
    "res.append([\"Sí\",\"Sí\", macro_F1_svm_mean_vector_sin_stop_words_pre_proc_agrandado])\n",
    "\n",
    "res.append([\"Sí\",\"No\", macro_F1_svm_mean_vector_sin_stop_words_agrandado])\n",
    "\n",
    "res.append([\"No\",\"Sí\", macro_F1_svm_mean_vector_sin_pre_proc_agrandado])\n",
    "\n",
    "res.append([\"No\",\"No\", macro_F1_svm_mean_vector_agrandado])\n",
    "\n",
    "print(\"Pruebas con Mean Vector entrenando en train_set+devel_set\")\n",
    "print(tabulate(res, headers=headers, tablefmt='fancy_grid'))\n",
    "\n",
    "macro_f1_con_mejores_hiperparametros = macro_F1_mean_vector(256,True,True, mejor_modelo, test_set)\n",
    "print(\"Macro F1 probado en test_set con los hiperparámetros mejorados: \",macro_f1_con_mejores_hiperparametros)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desglose de la mejor estrategia (SVM Mean Vector) mostrando su comportamiento en cada clase (métrica: exactitud)\n",
      "╒════════════╤═════════════╕\n",
      "│ Conjunto   │   Exactitud │\n",
      "╞════════════╪═════════════╡\n",
      "│ Positivos  │    0.658842 │\n",
      "├────────────┼─────────────┤\n",
      "│ Negativos  │    0.676871 │\n",
      "├────────────┼─────────────┤\n",
      "│ Neutros    │    0.497717 │\n",
      "╘════════════╧═════════════╛\n"
     ]
    }
   ],
   "source": [
    "# Puntuación por clase\n",
    "headers = ['Conjunto','Exactitud']\n",
    "res = []\n",
    "\n",
    "def obtener_tuits_por_clase(corpus):\n",
    "    positivos = []\n",
    "    negativos = []\n",
    "    neutros = []\n",
    "    for tuit in corpus:\n",
    "        if tuit[2] == 'P':\n",
    "            positivos.append(tuit)\n",
    "        if tuit[2] == 'N':\n",
    "            negativos.append(tuit)\n",
    "        if tuit[2] == 'NONE':\n",
    "            neutros.append(tuit)\n",
    "    return positivos, negativos, neutros\n",
    "\n",
    "tuits_positivos, tuits_negativos, tuits_neutros = obtener_tuits_por_clase(test_set)\n",
    "accuracy_positivos= accuracy_score_mean_vector(cantidad_entradas_vec_tuit,False,True,clf_svm_mean_vector, tuits_positivos)\n",
    "accuracy_negativos = accuracy_score_mean_vector(cantidad_entradas_vec_tuit,False,True,clf_svm_mean_vector, tuits_negativos)\n",
    "accuracy_neutros = accuracy_score_mean_vector(cantidad_entradas_vec_tuit,False,True,clf_svm_mean_vector, tuits_neutros)\n",
    "\n",
    "print(\"Desglose de la mejor estrategia (SVM Mean Vector) mostrando su comportamiento en cada clase (métrica: exactitud)\")\n",
    "res.append([\"Positivos\", accuracy_positivos])\n",
    "res.append([\"Negativos\", accuracy_negativos])\n",
    "res.append([\"Neutros\", accuracy_neutros])\n",
    "\n",
    "\n",
    "print(tabulate(res, headers=headers, tablefmt='fancy_grid'))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Ctp40-YxvUOM",
    "abBp1Qmsdnsq"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
